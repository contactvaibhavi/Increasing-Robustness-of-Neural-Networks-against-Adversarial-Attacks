# Increasing-Robustness-of-Neural-Networks-against-Adversarial-Attacks
Devising an effective Defense mechanism to enhance the Robustness of Neural Networks against Adversarial Attacks

## Introduction:
 Several machine learning models, including neural networks, consistently misclassify adversarial examplesâ€”inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Classifiers based on modern machine learning techniques, even those that obtain excellent performance on the test set, are not learning the true underlying concepts that determine the correct output label. An example generated for one model is often misclassified by other models, even when they have different architectures or were trained on disjoint training sets. Moreover, when these different models misclassify an adversarial example, they often agree with each other on its class. In this project, we analyse the representation of image features in neural networks and how it can be exploited by adversarial inputs to produce erroneous classification as the output. The work on the robustness of Neural Networks against Adversarial Attacks is an important area of research today with major contributions by top researchers in the field of Deep Learning. As we move towards a world that is increasingly being touched by innovations in Computer Vision, the Robustness of neural networks is very central to the efficacy of the models.

## Background Work

| S. No. | Title of Paper                                 | Main Contribution                     | Authors                                | Year |
|--------|------------------------------------------------|---------------------------------------|----------------------------------------|------|
| 1      | Intriguing Properties of Neural Networks       | Adversarial Attacks                   | Christian Szegedy                      | 2013 |
| 2      | Explaining and Harnessing Adversarial Examples | FSGM                                  | Ian Goodfellow                         | 2015 |
| 3      | Practical black-Box Attacks                    | Black-Box Attacks                     | Papernot                               | 2016 |
| 4      | Boosting Adversarial Attacks with Momentum     | BI-FSGM                               | Dong et. al Tsinghua University, China | 2018 |
| 5      | Evaluating Robustness of Neural Networks       | CLEVER: Extreme value Theory Approach | IBM                                    | 2018 |

## Technologies: 
 Convolutional Neural Networks, Capsule Networks, Adversarial Training, Adversarial inputs, FSGM etc.

